{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3acabbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install TensorFlow\n",
    "# !pip install -q tensorflow-gpu==2.0.0-beta1\n",
    "\n",
    "try:\n",
    "  %tensorflow_version 2.x  # Colab only.\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "`%tensorflow_version` only switches the major version: `1.x` or `2.x`.\n",
    "You set: `2.x  # Colab only.`. This will be interpreted as: `2.x`.\n",
    "\n",
    "\n",
    "TensorFlow 2.x selected.\n",
    "2.0.0-beta1\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# Just a simple test\n",
    "sentences = [\n",
    "    \"I like eggs and ham.\",\n",
    "    \"I love chocolate and bunnies.\",\n",
    "    \"I hate onions.\"\n",
    "]\n",
    "MAX_VOCAB_SIZE = 20000\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "print(sequences)\n",
    "[[1, 3, 4, 2, 5], [1, 6, 7, 2, 8], [1, 9, 10]]\n",
    "# How to get the word to index mapping?\n",
    "tokenizer.word_index\n",
    "{'and': 2,\n",
    " 'bunnies': 8,\n",
    " 'chocolate': 7,\n",
    " 'eggs': 4,\n",
    " 'ham': 5,\n",
    " 'hate': 9,\n",
    " 'i': 1,\n",
    " 'like': 3,\n",
    " 'love': 6,\n",
    " 'onions': 10}\n",
    "# use the defaults\n",
    "data = pad_sequences(sequences)\n",
    "print(data)\n",
    "[[ 1  3  4  2  5]\n",
    " [ 1  6  7  2  8]\n",
    " [ 0  0  1  9 10]]\n",
    "MAX_SEQUENCE_LENGTH = 5\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print(data)\n",
    "[[ 1  3  4  2  5]\n",
    " [ 1  6  7  2  8]\n",
    " [ 0  0  1  9 10]]\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "print(data)\n",
    "[[ 1  3  4  2  5]\n",
    " [ 1  6  7  2  8]\n",
    " [ 1  9 10  0  0]]\n",
    "# too much padding\n",
    "data = pad_sequences(sequences, maxlen=6)\n",
    "print(data)\n",
    "[[ 0  1  3  4  2  5]\n",
    " [ 0  1  6  7  2  8]\n",
    " [ 0  0  0  1  9 10]]\n",
    "# truncation\n",
    "data = pad_sequences(sequences, maxlen=4)\n",
    "print(data)\n",
    "[[ 3  4  2  5]\n",
    " [ 6  7  2  8]\n",
    " [ 0  1  9 10]]\n",
    "data = pad_sequences(sequences, maxlen=4, truncating='post')\n",
    "print(data)\n",
    "[[ 1  3  4  2]\n",
    " [ 1  6  7  2]\n",
    " [ 0  1  9 10]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975adc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install TensorFlow\n",
    "# !pip install -q tensorflow-gpu==2.0.0-beta1\n",
    "\n",
    "try:\n",
    "  %tensorflow_version 2.x  # Colab only.\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "`%tensorflow_version` only switches the major version: `1.x` or `2.x`.\n",
    "You set: `2.x  # Colab only.`. This will be interpreted as: `2.x`.\n",
    "\n",
    "\n",
    "TensorFlow 2.x selected.\n",
    "2.0.0-beta1\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from tensorflow.keras.models import Model\n",
    "# Unfortunately this URL doesn't work directly with pd.read_csv\n",
    "!wget -nc https://lazyprogrammer.me/course_files/spam.csv\n",
    "File ‘spam.csv’ already there; not retrieving.\n",
    "\n",
    "df = pd.read_csv('spam.csv', encoding='ISO-8859-1')\n",
    "df.head()\n",
    "v1\tv2\tUnnamed: 2\tUnnamed: 3\tUnnamed: 4\n",
    "0\tham\tGo until jurong point, crazy.. Available only ...\tNaN\tNaN\tNaN\n",
    "1\tham\tOk lar... Joking wif u oni...\tNaN\tNaN\tNaN\n",
    "2\tspam\tFree entry in 2 a wkly comp to win FA Cup fina...\tNaN\tNaN\tNaN\n",
    "3\tham\tU dun say so early hor... U c already then say...\tNaN\tNaN\tNaN\n",
    "4\tham\tNah I don't think he goes to usf, he lives aro...\tNaN\tNaN\tNaN\n",
    "# drop unnecessary columns\n",
    "df = df.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\n",
    "df.head()\n",
    "v1\tv2\n",
    "0\tham\tGo until jurong point, crazy.. Available only ...\n",
    "1\tham\tOk lar... Joking wif u oni...\n",
    "2\tspam\tFree entry in 2 a wkly comp to win FA Cup fina...\n",
    "3\tham\tU dun say so early hor... U c already then say...\n",
    "4\tham\tNah I don't think he goes to usf, he lives aro...\n",
    "# rename columns to something better\n",
    "df.columns = ['labels', 'data']\n",
    "df.head()\n",
    "labels\tdata\n",
    "0\tham\tGo until jurong point, crazy.. Available only ...\n",
    "1\tham\tOk lar... Joking wif u oni...\n",
    "2\tspam\tFree entry in 2 a wkly comp to win FA Cup fina...\n",
    "3\tham\tU dun say so early hor... U c already then say...\n",
    "4\tham\tNah I don't think he goes to usf, he lives aro...\n",
    "# create binary labels\n",
    "df['b_labels'] = df['labels'].map({'ham': 0, 'spam': 1})\n",
    "Y = df['b_labels'].values\n",
    "# split up the data\n",
    "df_train, df_test, Ytrain, Ytest = train_test_split(df['data'], Y, test_size=0.33)\n",
    "# Convert sentences to sequences\n",
    "MAX_VOCAB_SIZE = 20000\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(df_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(df_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(df_test)\n",
    "# get word -> integer mapping\n",
    "word2idx = tokenizer.word_index\n",
    "V = len(word2idx)\n",
    "print('Found %s unique tokens.' % V)\n",
    "Found 7257 unique tokens.\n",
    "# pad sequences so that we get a N x T matrix\n",
    "data_train = pad_sequences(sequences_train)\n",
    "print('Shape of data train tensor:', data_train.shape)\n",
    "\n",
    "# get sequence length\n",
    "T = data_train.shape[1]\n",
    "Shape of data train tensor: (3733, 121)\n",
    "data_test = pad_sequences(sequences_test, maxlen=T)\n",
    "print('Shape of data test tensor:', data_test.shape)\n",
    "Shape of data test tensor: (1839, 121)\n",
    "# Create the model\n",
    "\n",
    "# We get to choose embedding dimensionality\n",
    "D = 20\n",
    "\n",
    "# Note: we actually want to the size of the embedding to (V + 1) x D,\n",
    "# because the first index starts from 1 and not 0.\n",
    "# Thus, if the final index of the embedding matrix is V,\n",
    "# then it actually must have size V + 1.\n",
    "\n",
    "i = Input(shape=(T,))\n",
    "x = Embedding(V + 1, D)(i)\n",
    "x = Conv1D(32, 3, activation='relu')(x)\n",
    "x = MaxPooling1D(3)(x)\n",
    "x = Conv1D(64, 3, activation='relu')(x)\n",
    "x = MaxPooling1D(3)(x)\n",
    "x = Conv1D(128, 3, activation='relu')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(i, x)\n",
    "# Compile and fit\n",
    "model.compile(\n",
    "  loss='binary_crossentropy',\n",
    "  optimizer='adam',\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "\n",
    "\n",
    "print('Training model...')\n",
    "r = model.fit(\n",
    "  data_train,\n",
    "  Ytrain,\n",
    "  epochs=5,\n",
    "  validation_data=(data_test, Ytest)\n",
    ")\n",
    "WARNING: Logging before flag parsing goes to stderr.\n",
    "W0817 17:10:21.218855 140024925730688 deprecation.py:323] From /tensorflow-2.0.0b1/python3.6/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
    "Instructions for updating:\n",
    "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
    "Training model...\n",
    "Train on 3733 samples, validate on 1839 samples\n",
    "Epoch 1/5\n",
    "3733/3733 [==============================] - 5s 1ms/sample - loss: 0.4038 - accuracy: 0.8647 - val_loss: 0.3462 - val_accuracy: 0.8684\n",
    "Epoch 2/5\n",
    "3733/3733 [==============================] - 1s 334us/sample - loss: 0.2440 - accuracy: 0.8947 - val_loss: 0.1272 - val_accuracy: 0.9706\n",
    "Epoch 3/5\n",
    "3733/3733 [==============================] - 1s 330us/sample - loss: 0.0454 - accuracy: 0.9874 - val_loss: 0.0939 - val_accuracy: 0.9777\n",
    "Epoch 4/5\n",
    "3733/3733 [==============================] - 1s 331us/sample - loss: 0.0223 - accuracy: 0.9941 - val_loss: 0.1070 - val_accuracy: 0.9810\n",
    "Epoch 5/5\n",
    "3733/3733 [==============================] - 1s 331us/sample - loss: 0.0141 - accuracy: 0.9960 - val_loss: 0.1020 - val_accuracy: 0.9793\n",
    "# Plot loss per iteration\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(r.history['loss'], label='loss')\n",
    "plt.plot(r.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "<matplotlib.legend.Legend at 0x7f595ac7a9b0>\n",
    "\n",
    "# Plot accuracy per iteration\n",
    "plt.plot(r.history['accuracy'], label='acc')\n",
    "plt.plot(r.history['val_accuracy'], label='val_acc')\n",
    "plt.legend()\n",
    "<matplotlib.legend.Legend at 0x7f5958430898>\n",
    "\n",
    "ssssss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
