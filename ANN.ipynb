{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f23d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install TensorFlow\n",
    "# !pip install -q tensorflow-gpu==2.0.0-beta1\n",
    "\n",
    "try:\n",
    "  %tensorflow_version 2.x  # Colab only.\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "     |████████████████████████████████| 348.9MB 52kB/s \n",
    "     |████████████████████████████████| 3.1MB 49.1MB/s \n",
    "     |████████████████████████████████| 501kB 54.6MB/s \n",
    "2.0.0-beta1\n",
    "# Load in the data\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "print(\"x_train.shape:\", x_train.shape)\n",
    "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
    "11493376/11490434 [==============================] - 0s 0us/step\n",
    "x_train.shape: (60000, 28, 28)\n",
    "# Build the model\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "# Train the model\n",
    "r = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10)\n",
    "WARNING: Logging before flag parsing goes to stderr.\n",
    "W0718 16:30:06.976897 139639418169216 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
    "Instructions for updating:\n",
    "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
    "Train on 60000 samples, validate on 10000 samples\n",
    "Epoch 1/10\n",
    "60000/60000 [==============================] - 7s 111us/sample - loss: 0.2949 - accuracy: 0.9134 - val_loss: 0.1339 - val_accuracy: 0.9594\n",
    "Epoch 2/10\n",
    "60000/60000 [==============================] - 6s 93us/sample - loss: 0.1452 - accuracy: 0.9564 - val_loss: 0.1063 - val_accuracy: 0.9681\n",
    "Epoch 3/10\n",
    "60000/60000 [==============================] - 6s 95us/sample - loss: 0.1100 - accuracy: 0.9664 - val_loss: 0.0879 - val_accuracy: 0.9722\n",
    "Epoch 4/10\n",
    "60000/60000 [==============================] - 6s 94us/sample - loss: 0.0902 - accuracy: 0.9721 - val_loss: 0.0737 - val_accuracy: 0.9766\n",
    "Epoch 5/10\n",
    "60000/60000 [==============================] - 6s 93us/sample - loss: 0.0765 - accuracy: 0.9758 - val_loss: 0.0747 - val_accuracy: 0.9748\n",
    "Epoch 6/10\n",
    "60000/60000 [==============================] - 6s 94us/sample - loss: 0.0672 - accuracy: 0.9789 - val_loss: 0.0699 - val_accuracy: 0.9772\n",
    "Epoch 7/10\n",
    "60000/60000 [==============================] - 6s 93us/sample - loss: 0.0586 - accuracy: 0.9820 - val_loss: 0.0729 - val_accuracy: 0.9781\n",
    "Epoch 8/10\n",
    "60000/60000 [==============================] - 6s 94us/sample - loss: 0.0544 - accuracy: 0.9815 - val_loss: 0.0697 - val_accuracy: 0.9780\n",
    "Epoch 9/10\n",
    "60000/60000 [==============================] - 6s 93us/sample - loss: 0.0505 - accuracy: 0.9834 - val_loss: 0.0690 - val_accuracy: 0.9789\n",
    "Epoch 10/10\n",
    "60000/60000 [==============================] - 6s 93us/sample - loss: 0.0447 - accuracy: 0.9850 - val_loss: 0.0692 - val_accuracy: 0.9806\n",
    "# Plot loss per iteration\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(r.history['loss'], label='loss')\n",
    "plt.plot(r.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "<matplotlib.legend.Legend at 0x7effe00f2ac8>\n",
    "\n",
    "# Plot accuracy per iteration\n",
    "plt.plot(r.history['accuracy'], label='acc')\n",
    "plt.plot(r.history['val_accuracy'], label='val_acc')\n",
    "plt.legend()\n",
    "<matplotlib.legend.Legend at 0x7effe0092518>\n",
    "\n",
    "# Evaluate the model\n",
    "print(model.evaluate(x_test, y_test))\n",
    "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0692 - accuracy: 0.9806\n",
    "[0.06924617666350968, 0.9806]\n",
    "# Plot confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "  \"\"\"\n",
    "  This function prints and plots the confusion matrix.\n",
    "  Normalization can be applied by setting `normalize=True`.\n",
    "  \"\"\"\n",
    "  if normalize:\n",
    "      cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "      print(\"Normalized confusion matrix\")\n",
    "  else:\n",
    "      print('Confusion matrix, without normalization')\n",
    "\n",
    "  print(cm)\n",
    "\n",
    "  plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "  plt.title(title)\n",
    "  plt.colorbar()\n",
    "  tick_marks = np.arange(len(classes))\n",
    "  plt.xticks(tick_marks, classes, rotation=45)\n",
    "  plt.yticks(tick_marks, classes)\n",
    "\n",
    "  fmt = '.2f' if normalize else 'd'\n",
    "  thresh = cm.max() / 2.\n",
    "  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "      plt.text(j, i, format(cm[i, j], fmt),\n",
    "               horizontalalignment=\"center\",\n",
    "               color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.ylabel('True label')\n",
    "  plt.xlabel('Predicted label')\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "p_test = model.predict(x_test).argmax(axis=1)\n",
    "cm = confusion_matrix(y_test, p_test)\n",
    "plot_confusion_matrix(cm, list(range(10)))\n",
    "\n",
    "# Do these results make sense?\n",
    "# It's easy to confuse 9 <--> 4, 9 <--> 7, 2 <--> 7, etc. \n",
    "Confusion matrix, without normalization\n",
    "[[ 972    0    1    1    0    0    2    1    2    1]\n",
    " [   0 1126    2    2    0    0    2    0    3    0]\n",
    " [   2    1 1015    3    1    0    1    4    4    1]\n",
    " [   1    0    1  996    0    2    0    3    3    4]\n",
    " [   1    1    2    1  959    0    5    3    1    9]\n",
    " [   2    1    0   12    1  868    2    2    3    1]\n",
    " [   5    3    1    1    1    3  939    0    5    0]\n",
    " [   2   10   12    3    0    0    0  994    3    4]\n",
    " [   6    0    3    2    3    1    0    2  953    4]\n",
    " [   2    2    0    3   12    3    0    3    0  984]]\n",
    "\n",
    "# Show some misclassified examples\n",
    "misclassified_idx = np.where(p_test != y_test)[0]\n",
    "i = np.random.choice(misclassified_idx)\n",
    "plt.imshow(x_test[i], cmap='gray')\n",
    "plt.title(\"True label: %s Predicted: %s\" % (y_test[i], p_test[i]));\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c35eb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install TensorFlow\n",
    "# !pip install -q tensorflow-gpu==2.0.0-beta1\n",
    "\n",
    "try:\n",
    "  %tensorflow_version 2.x  # Colab only.\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "2.0.0-beta1\n",
    "# Other imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "# Make the dataset\n",
    "N = 1000\n",
    "X = np.random.random((N, 2)) * 6 - 3 # uniformly distributed between (-3, +3)\n",
    "Y = np.cos(2*X[:,0]) + np.cos(3*X[:,1])\n",
    "This implements the function:\n",
    "\n",
    "y\n",
    "=\n",
    "cos\n",
    "(\n",
    "2\n",
    "x\n",
    "1\n",
    ")\n",
    "+\n",
    "c\n",
    "o\n",
    "s\n",
    "(\n",
    "3\n",
    "x\n",
    "2\n",
    ")\n",
    "# Plot it\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X[:,0], X[:,1], Y)\n",
    "# plt.show()\n",
    "<mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7f2e953549e8>\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Dense(128, input_shape=(2,), activation='relu'),\n",
    "  tf.keras.layers.Dense(1)\n",
    "])\n",
    "# Compile and fit\n",
    "opt = tf.keras.optimizers.Adam(0.01)\n",
    "model.compile(optimizer=opt, loss='mse')\n",
    "r = model.fit(X, Y, epochs=100)\n",
    "Train on 1000 samples\n",
    "Epoch 1/100\n",
    "1000/1000 [==============================] - 0s 493us/sample - loss: 0.9276\n",
    "Epoch 2/100\n",
    "1000/1000 [==============================] - 0s 58us/sample - loss: 0.9060\n",
    "Epoch 3/100\n",
    "1000/1000 [==============================] - 0s 68us/sample - loss: 0.8808\n",
    "Epoch 4/100\n",
    "1000/1000 [==============================] - 0s 54us/sample - loss: 0.8457\n",
    "Epoch 5/100\n",
    "1000/1000 [==============================] - 0s 57us/sample - loss: 0.8115\n",
    "Epoch 6/100\n",
    "1000/1000 [==============================] - 0s 59us/sample - loss: 0.7682\n",
    "Epoch 7/100\n",
    "1000/1000 [==============================] - 0s 59us/sample - loss: 0.6904\n",
    "Epoch 8/100\n",
    "1000/1000 [==============================] - 0s 58us/sample - loss: 0.6319\n",
    "Epoch 9/100\n",
    "1000/1000 [==============================] - 0s 51us/sample - loss: 0.5543\n",
    "Epoch 10/100\n",
    "1000/1000 [==============================] - 0s 58us/sample - loss: 0.5207\n",
    "Epoch 11/100\n",
    "1000/1000 [==============================] - 0s 54us/sample - loss: 0.4973\n",
    "Epoch 12/100\n",
    "1000/1000 [==============================] - 0s 52us/sample - loss: 0.4889\n",
    "Epoch 13/100\n",
    "1000/1000 [==============================] - 0s 58us/sample - loss: 0.4715\n",
    "Epoch 14/100\n",
    "1000/1000 [==============================] - 0s 55us/sample - loss: 0.4714\n",
    "Epoch 15/100\n",
    "1000/1000 [==============================] - 0s 53us/sample - loss: 0.4387\n",
    "Epoch 16/100\n",
    "1000/1000 [==============================] - 0s 58us/sample - loss: 0.4519\n",
    "Epoch 17/100\n",
    "1000/1000 [==============================] - 0s 53us/sample - loss: 0.4498\n",
    "Epoch 18/100\n",
    "1000/1000 [==============================] - 0s 53us/sample - loss: 0.4180\n",
    "Epoch 19/100\n",
    "1000/1000 [==============================] - 0s 53us/sample - loss: 0.4163\n",
    "Epoch 20/100\n",
    "1000/1000 [==============================] - 0s 54us/sample - loss: 0.4128\n",
    "Epoch 21/100\n",
    "1000/1000 [==============================] - 0s 67us/sample - loss: 0.4110\n",
    "Epoch 22/100\n",
    "1000/1000 [==============================] - 0s 62us/sample - loss: 0.3984\n",
    "Epoch 23/100\n",
    "1000/1000 [==============================] - 0s 58us/sample - loss: 0.3825\n",
    "Epoch 24/100\n",
    "1000/1000 [==============================] - 0s 52us/sample - loss: 0.3734\n",
    "Epoch 25/100\n",
    "1000/1000 [==============================] - 0s 54us/sample - loss: 0.4398\n",
    "Epoch 26/100\n",
    "1000/1000 [==============================] - 0s 57us/sample - loss: 0.3796\n",
    "Epoch 27/100\n",
    "1000/1000 [==============================] - 0s 53us/sample - loss: 0.3603\n",
    "Epoch 28/100\n",
    "1000/1000 [==============================] - 0s 54us/sample - loss: 0.3120\n",
    "Epoch 29/100\n",
    "1000/1000 [==============================] - 0s 58us/sample - loss: 0.2797\n",
    "Epoch 30/100\n",
    "1000/1000 [==============================] - 0s 53us/sample - loss: 0.2815\n",
    "Epoch 31/100\n",
    "1000/1000 [==============================] - 0s 54us/sample - loss: 0.2600\n",
    "Epoch 32/100\n",
    "1000/1000 [==============================] - 0s 54us/sample - loss: 0.2221\n",
    "Epoch 33/100\n",
    "1000/1000 [==============================] - 0s 52us/sample - loss: 0.2036\n",
    "Epoch 34/100\n",
    "1000/1000 [==============================] - 0s 59us/sample - loss: 0.1905\n",
    "Epoch 35/100\n",
    "1000/1000 [==============================] - 0s 54us/sample - loss: 0.1502\n",
    "Epoch 36/100\n",
    "1000/1000 [==============================] - 0s 51us/sample - loss: 0.1368\n",
    "Epoch 37/100\n",
    "1000/1000 [==============================] - 0s 51us/sample - loss: 0.1300\n",
    "Epoch 38/100\n",
    "1000/1000 [==============================] - 0s 52us/sample - loss: 0.1043\n",
    "Epoch 39/100\n",
    "1000/1000 [==============================] - 0s 66us/sample - loss: 0.1022\n",
    "Epoch 40/100\n",
    "1000/1000 [==============================] - 0s 49us/sample - loss: 0.0823\n",
    "Epoch 41/100\n",
    "1000/1000 [==============================] - 0s 49us/sample - loss: 0.0787\n",
    "Epoch 42/100\n",
    "1000/1000 [==============================] - 0s 51us/sample - loss: 0.0593\n",
    "Epoch 43/100\n",
    "1000/1000 [==============================] - 0s 53us/sample - loss: 0.0528\n",
    "Epoch 44/100\n",
    "1000/1000 [==============================] - 0s 48us/sample - loss: 0.0616\n",
    "Epoch 45/100\n",
    "1000/1000 [==============================] - 0s 50us/sample - loss: 0.0351\n",
    "Epoch 46/100\n",
    "1000/1000 [==============================] - 0s 50us/sample - loss: 0.0273\n",
    "Epoch 47/100\n",
    "1000/1000 [==============================] - 0s 49us/sample - loss: 0.0288\n",
    "Epoch 48/100\n",
    "1000/1000 [==============================] - 0s 51us/sample - loss: 0.0281\n",
    "Epoch 49/100\n",
    "1000/1000 [==============================] - 0s 49us/sample - loss: 0.0362\n",
    "Epoch 50/100\n",
    "1000/1000 [==============================] - 0s 48us/sample - loss: 0.0585\n",
    "Epoch 51/100\n",
    "1000/1000 [==============================] - 0s 52us/sample - loss: 0.0842\n",
    "Epoch 52/100\n",
    "1000/1000 [==============================] - 0s 54us/sample - loss: 0.0780\n",
    "Epoch 53/100\n",
    "1000/1000 [==============================] - 0s 53us/sample - loss: 0.0700\n",
    "Epoch 54/100\n",
    "1000/1000 [==============================] - 0s 49us/sample - loss: 0.0232\n",
    "Epoch 55/100\n",
    "1000/1000 [==============================] - 0s 50us/sample - loss: 0.0149\n",
    "Epoch 56/100\n",
    "1000/1000 [==============================] - 0s 49us/sample - loss: 0.0159\n",
    "Epoch 57/100\n",
    "1000/1000 [==============================] - 0s 49us/sample - loss: 0.0140\n",
    "Epoch 58/100\n",
    "1000/1000 [==============================] - 0s 52us/sample - loss: 0.0108\n",
    "Epoch 59/100\n",
    "1000/1000 [==============================] - 0s 66us/sample - loss: 0.0088\n",
    "Epoch 60/100\n",
    "1000/1000 [==============================] - 0s 52us/sample - loss: 0.0077\n",
    "Epoch 61/100\n",
    "1000/1000 [==============================] - 0s 50us/sample - loss: 0.0132\n",
    "Epoch 62/100\n",
    "1000/1000 [==============================] - 0s 50us/sample - loss: 0.0108\n",
    "Epoch 63/100\n",
    "1000/1000 [==============================] - 0s 50us/sample - loss: 0.0105\n",
    "Epoch 64/100\n",
    "1000/1000 [==============================] - 0s 53us/sample - loss: 0.0077\n",
    "Epoch 65/100\n",
    "1000/1000 [==============================] - 0s 51us/sample - loss: 0.0084\n",
    "Epoch 66/100\n",
    "1000/1000 [==============================] - 0s 50us/sample - loss: 0.0101\n",
    "Epoch 67/100\n",
    "1000/1000 [==============================] - 0s 49us/sample - loss: 0.0093\n",
    "Epoch 68/100\n",
    "1000/1000 [==============================] - 0s 49us/sample - loss: 0.0082\n",
    "Epoch 69/100\n",
    "1000/1000 [==============================] - 0s 49us/sample - loss: 0.0076\n",
    "Epoch 70/100\n",
    "1000/1000 [==============================] - 0s 58us/sample - loss: 0.0145\n",
    "Epoch 71/100\n",
    "1000/1000 [==============================] - 0s 47us/sample - loss: 0.0127\n",
    "Epoch 72/100\n",
    "1000/1000 [==============================] - 0s 50us/sample - loss: 0.0101\n",
    "Epoch 73/100\n",
    "1000/1000 [==============================] - 0s 49us/sample - loss: 0.0058\n",
    "Epoch 74/100\n",
    "1000/1000 [==============================] - 0s 50us/sample - loss: 0.0051\n",
    "Epoch 75/100\n",
    "1000/1000 [==============================] - 0s 53us/sample - loss: 0.0053\n",
    "Epoch 76/100\n",
    "1000/1000 [==============================] - 0s 52us/sample - loss: 0.0073\n",
    "Epoch 77/100\n",
    "1000/1000 [==============================] - 0s 50us/sample - loss: 0.0065\n",
    "Epoch 78/100\n",
    "1000/1000 [==============================] - 0s 53us/sample - loss: 0.0066\n",
    "Epoch 79/100\n",
    "1000/1000 [==============================] - 0s 60us/sample - loss: 0.0202\n",
    "Epoch 80/100\n",
    "1000/1000 [==============================] - 0s 49us/sample - loss: 0.0130\n",
    "Epoch 81/100\n",
    "1000/1000 [==============================] - 0s 50us/sample - loss: 0.0093\n",
    "Epoch 82/100\n",
    "1000/1000 [==============================] - 0s 49us/sample - loss: 0.0149\n",
    "Epoch 83/100\n",
    "1000/1000 [==============================] - 0s 50us/sample - loss: 0.0147\n",
    "Epoch 84/100\n",
    "1000/1000 [==============================] - 0s 46us/sample - loss: 0.0085\n",
    "Epoch 85/100\n",
    "1000/1000 [==============================] - 0s 49us/sample - loss: 0.0048\n",
    "Epoch 86/100\n",
    "1000/1000 [==============================] - 0s 51us/sample - loss: 0.0041\n",
    "Epoch 87/100\n",
    "1000/1000 [==============================] - 0s 48us/sample - loss: 0.0041\n",
    "Epoch 88/100\n",
    "1000/1000 [==============================] - 0s 50us/sample - loss: 0.0106\n",
    "Epoch 89/100\n",
    "1000/1000 [==============================] - 0s 50us/sample - loss: 0.0112\n",
    "Epoch 90/100\n",
    "1000/1000 [==============================] - 0s 47us/sample - loss: 0.0051\n",
    "Epoch 91/100\n",
    "1000/1000 [==============================] - 0s 48us/sample - loss: 0.0121\n",
    "Epoch 92/100\n",
    "1000/1000 [==============================] - 0s 52us/sample - loss: 0.0384\n",
    "Epoch 93/100\n",
    "1000/1000 [==============================] - 0s 51us/sample - loss: 0.0463\n",
    "Epoch 94/100\n",
    "1000/1000 [==============================] - 0s 49us/sample - loss: 0.0190\n",
    "Epoch 95/100\n",
    "1000/1000 [==============================] - 0s 49us/sample - loss: 0.0340\n",
    "Epoch 96/100\n",
    "1000/1000 [==============================] - 0s 52us/sample - loss: 0.0133\n",
    "Epoch 97/100\n",
    "1000/1000 [==============================] - 0s 49us/sample - loss: 0.0062\n",
    "Epoch 98/100\n",
    "1000/1000 [==============================] - 0s 67us/sample - loss: 0.0043\n",
    "Epoch 99/100\n",
    "1000/1000 [==============================] - 0s 67us/sample - loss: 0.0050\n",
    "Epoch 100/100\n",
    "1000/1000 [==============================] - 0s 51us/sample - loss: 0.0045\n",
    "# Plot the loss\n",
    "plt.plot(r.history['loss'], label='loss')\n",
    "[<matplotlib.lines.Line2D at 0x7f2e95406a58>]\n",
    "\n",
    "# Plot the prediction surface\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X[:,0], X[:,1], Y)\n",
    "\n",
    "# surface plot\n",
    "line = np.linspace(-3, 3, 50)\n",
    "xx, yy = np.meshgrid(line, line)\n",
    "Xgrid = np.vstack((xx.flatten(), yy.flatten())).T\n",
    "Yhat = model.predict(Xgrid).flatten()\n",
    "ax.plot_trisurf(Xgrid[:,0], Xgrid[:,1], Yhat, linewidth=0.2, antialiased=True)\n",
    "plt.show()\n",
    "\n",
    "# Can it extrapolate?\n",
    "# Plot the prediction surface\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X[:,0], X[:,1], Y)\n",
    "\n",
    "# surface plot\n",
    "line = np.linspace(-5, 5, 50)\n",
    "xx, yy = np.meshgrid(line, line)\n",
    "Xgrid = np.vstack((xx.flatten(), yy.flatten())).T\n",
    "Yhat = model.predict(Xgrid).flatten()\n",
    "ax.plot_trisurf(Xgrid[:,0], Xgrid[:,1], Yhat, linewidth=0.2, antialiased=True)\n",
    "plt.show()\n",
    "\n",
    "sss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
